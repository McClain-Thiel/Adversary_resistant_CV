<html class="w-100">
  <head>
    <!-- Global site tag (gtag.js) -->
      <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
      <script async src="https://www.googletagmanager.com/gtag/js?id=UA-157519291-1"></script>
      <script async src="assets/idk_js.js"></script>
      <link href="assets/idk_css.css" rel="stylesheet">
      <link href="assets/bootstrap.min.css" rel="stylesheet">
      <link rel="stylesheet" type="text/css" href="assets/stylebiblio.css">
      <script type="text/javascript" src="assets/bibtextohtml.js"></script>
      <title>Robust CV</title>
  </head>

  <body class="w-100">
  <div class="row w-100 justify-content-center">
    <div class="col-6 text-left">
      <h2>Robust Computer vision</h2>
    </div>
  </div>
  <div class="row w-100 justify-content-center">
    <div class="col-6 text-left my-auto">
      <h4>Dante Everaert, McClain Thiel, Greg Yannett</h4>
        <h5>For CS182: Designing, Visualizing and Understanding Deep Neural Networks </h5>
    </div>
  </div>

  <!--Background / problem statment -->
    <div class="row w-100 justify-content-center pt-5" id="overview">
      <div class="col-6 text-left">
          <h2>Background</h2>
        <p>
            Over the course of the past 10-15 years, machine learning, and more specifically computer vision (CV)
            has had a major impact on many industries and the consensus is that it will continue to disrupt and
            revolutionize more and more facets of everyday life. Already, CV systems have shown promise and even
            superhuman performance in areas ranging from driving to medical diagnosis. They are able to do this by
            leveraging massive amounts of data and complex algorithms that can be trained to complete a specific task,
            recently, however,  a serious problem has emerged. Look at the following example:
            <img class="center" src="assets/imgs/adversarial-example.jpg" alt="me annotaed">

            To a human, theses look identical. Same animal, same pose, same lighting, etc. But to a state of the art
            neural net, the image on the right looks like a gibbon. This net, which routinely outperforms humans, is
            more than 99% sure that the picture on the right is a gibbon. What could be causing this? </p>

          <p>CV systems don't
            'see' the same way humans do. If asked, "how do you identify a stop sign?", most humans would likely answer
            something along the lines of shape, color, and the word 'stop' written on it. Neural networks don't operate
            the same way, they look at features that aren't necessarily salient to the task but allow the model to
            pictures easily. In practice, this means that they often rely heavily on texture and other aspects that a
            human wouldn't consider the most relevant features for identifying an object. Clearly, this has some
            advantages, as evidenced by the system's performance on any number of standardized tasks, however, there are
            also significant drawbacks. The texture can be subtly changed or 'perturbed'  in such a way that it fools
            the system into thinking that a picture is something it is obviously not. This minor perturbation doesn't
            affect a human's ability to recognize a picture, and often isn't even noticeable, but it absolutely destroys
            a computer's ability to make sense of an image.
          </p>

          <p>
              These perturbations can be introduced to the system in two main ways: targeted or natural. Targeted
              adversarial perturbations are difficult to defend against especially if the attacker has access to the
              original model. There are many mature techniques for attacking theses systems and an effective defense
              against them is still an open problem in the field. Applications of these attacks can be dangerous in the
              right situation. Many self-driving cars rely at least partially on computer vision, which we know is
              vulnerable, so if a creative attacker managed to perturb a stop sign in a specific way, they could cause
              the car to perceive the sign as a 50 mph and accelerate instead of stopping.
              <img class="center" src="assets/imgs/stopsign.jpg" alt="me annotaed">

              There are many other applications of this approach but targeted attacks are largely out of scope for this writing.
          </p>

          <p>
              Our approach is a training method for defense against natural adversarial examples, which are broadly
              defined as images that aren't altered post-photography, but that cause significant and unexpected problems
              for CV applications. These are slightly easier to correct for as they are not designed to cause problems.
              <img class="center" src="assets/imgs/nat_example.jpg" alt="me annotaed">
              If we can identify and reproduce the problem, we can simply retrain the network with these examples as well
              as the original training set. This is called adversarial training. Adversarial training has been used for
              defending against targeted attacks as well but its efficacy is somewhat limited.
          </p>

          <p>
              In this repo, we develop an image recognition system based on res-net and the tiny image-net dataset
              that is somewhat resistant to random natural perturbations. We show that our training method is effective
              at improving the accuracy of our underlying model when testing on cross-validated tiny image-net data
              which consists of 100,000 64x64 RGB images spanning 200 classes for training and 10,000 similar images
              for validation.
          </p>

      </div>
    </div>

  <!-- Approach-->
  <div class="row w-100 justify-content-center pt-5" id="overview">
      <div class="col-6 text-left">
          <h2>Approach</h2>
          <h3>Data Preparation</h3>
          <p>
              As mentioned above, we are using the tiny-imagenet dataset. All of our data is dynamically augmented in two
              main ways. The first and most important modification is generating and applying perturbations. This process
              will be described in-depth in a future section. The second augmentation method is more conventional data
              augmentation such as applying simple transformations to the images in the data loader. We use TensorFlow's
              built-in ImageDataGenerator class to flip, shift, and zoom the images in a pseudo-random fashion as the
              data is transferred from the directory to the model.
          </p>
          <h3>Baseline Model</h3>
          <p>
              We originally implemented as relatively shallow CNN consisting of 3 layers each of which had a convolutional
              layer, a Relu non-linearity, and a max-pooling function, followed by a fully connected layer for the output.
              We quickly realized that this architecture was not going to perform as well as we needed.  We began
              researching state of the art models that had proven capable of superhuman performance. We settled on
              res-net, due to a combination of its impressive performance and relatively simple architecture. Due to our
              limited time and computational resources, a model that we could train and fine-tune quickly was essential
              .  Our implementation of Res-Net performed well on the public validation set with XX.X% accuracy on 10,000
              images. Below is a graphical representation of our implementation of Res-Net:
          </p>
          <img class="center" src="assets/imgs/resnet.jpg" alt="me annotaed">

          <h3>Adversarial Training</h3>
          <p>
              To defend against, adversarial examples found in nature, we first looked at what sort of images caused
              problems in the 'Natural Adversarial Examples' paper. We concluded that many of the examples that were
              misclassified could be broadly sorted into 3 types of problem images. The first type is just an out of
              distribution example. If the model hasn't ever seen a squid, it is not going to be able to recognize one
              at test time. We would love for the model to just admit it has no idea what it's looking at instead of
              making a high confidence guess but this is slightly out of scope. We aim to improve performance, not allow
              for elegant failure. Hardening against out of distribution examples is often just improving the dataset by
              making it bigger. Another type is a known object with a background or overlay of a texture that was
              confusing to the classifier. Here are some examples of that:
          </p>
              <div class="row justify-content-center">
                  <div class="col-sm-6 align-content-center">
                      <a href="#"><img src="assets/imgs/mesh_example.jpg" alt="inn_logo" class="sm-img"></a>
                  </div>
                  <div class="col-sm-6 justify-content-center">
                      <a href="#"><img src="assets/imgs/from_example.jpg" alt="ccs_logo" class="sm-img"></a>
                  </div>
              </div>
          <p>
              As mentioned above, ANNs are partial to texture, so this type of example causes some problems that are
              hard to avoid.  The third broad category is known objects but where the picture is taken from a slightly
              different angle. The training set might have a front view of a car, but the test picture might be from a
              traffic camera where the angle is different.
          </p>
          <p>
              This third type is much broader, consisting of any obstruction or modification that isn't covered by
              the first two. Some common examples are and object that is only partially in the frame or the object is
              zoomed in on and the net can't recognize it. In order to, train against these types of images, we
              synthetically generate  new images that are meant to mimic the same types of problems in the original
              training set. By freezing most of the network and retraining select unfrozen layers with our new dataset
              of perturbed images, we hope to mitigate the effect of adversarial examples.
          </p>

          <p>
              Our perturbations are listed below:
          </p>
              <ul>
                  <li>flip</li>
                  <li>zoom</li>
                  <li>rotate</li>
                  <li>scale</li>
                  <li>shift</li>
                  <li>sheer</li>
                  <li>brightness</li>
                  <li>dropout</li>
                  <li>partial delete</li>
                  <li>mesh overlay</li>
                  <li>blur</li>
               </ul>
          <p>
              Most of these are too simple or self explanatory to justify an explanation but examples are
              shown below:
          </p>
          <img class="center" src="assets/imgs/perturbs.jpg" alt="me annotaed">
          <p>
            All of these effects are applied to random pictures and to random degrees in the training set. For example,
              dropout was implemented such that a random percent of pixels between 5% and 15% would be deleted at random
              locations. Similar randomization procedures are implemented for all functions in an attempt to cover all
              reasonable natual perturbations.
          </p>

          <p>
              Once a suitable number of perturbed images are generated ( about half as many as the model was trained in
              per class), the original model is retrained with all layered except X, X, and X frozen. This prevents the
              network from overfitting and being able to classify exclusively perturbed examples. It is retrained for XX
              epochs with the entire dataset which includes about half original training images, and half synthetic
              adversarial examples.
          </p>

      </div>
  </div>

  <!-- Results-->
  <div class="row w-100 justify-content-center pt-5" id="overview">
      <div class="col-6 text-left">
          <h2>Results</h2>
          <p>
              training...
          </p>

      </div>
  </div>

  <!--tools -->

  <div class="row w-100 justify-content-center pt-5" id="overview">
      <div class="col-6 text-left">
          <h2>Tools</h2>
          <p>
              We implemented this project using the TensorFlow 1 python API. We settled on this framework for several
              reasons, primarily group members familiarity with certain aspects of the framework. We used the Keras
              high-level API to load the base model because we didn't need to implement any custom functionality in the
              network itself and the pre-trained network was designed to work with this version of Tensorflow.  The image
              perturbation process involved first writing all of the custom functions written above and then applying
              these functions in a stochastic fashion to out dataset. Once stored we froze most layers and retrained on
              the augmented images. The custom functions use several dependencies including common image manipulation
              libraries like NumPy, PIL, OpenCV, and imutils.
          </p>

          <p>
              We used Google Collaboratory as our primary development platform because it allows real-time collaboration
              with free synchronized data storage, and because it gave us access to cheap hardware accelerators without
              worrying about drivers and other low-level dependencies that are often issues on virtual machines.
          </p>


      </div>
  </div>

  <!-- lessons learned-->

  <div class="row w-100 justify-content-center pt-5" id="overview">
      <div class="col-6 text-left">
          <h2>Lessons Learned</h2>
          <p>
             hell if i know
          </p>


      </div>
  </div>

  <!-- team contributions-->

  <div class="row w-100 justify-content-center pt-5" id="overview">
      <div class="col-6 text-left">
          <h2>Team Contributions</h2>
          <p>
              i mean someone did some work
          </p>


      </div>
  </div>

  <!-- refrences-->

  <div class="row w-100 justify-content-center pt-5" id="overview">
      <div class="col-6 text-left">
          <h2>References</h2>
          <ol>
              <li>
                  Machine Learning @ Berkeley. "Tricking Neural Networks: Create Your Own Adversarial Examples."
                  Medium, Medium, 7 Mar. 2019, medium.com/@ml.at.berkeley/tricking-neural-networks-create-your-own-
                  adversarial-examples-a61eb7620fd8.
              </li>

              <li>
                  Gilmer, Justin, and Dan Hendrycks. "A Discussion of 'Adversarial Examples Are Not Bugs, They Are
                  Features': Adversarial Example Researchers Need to Expand What Is Meant by 'Robustness'." Distill,
                  vol. 4, no. 8, 2019, doi:10.23915/distill.00019.1.
              </li>

              <li>
                  Goodfellow, Ian. "Attacking Machine Learning with Adversarial Examples." OpenAI, OpenAI, 7 Mar. 2019,
                  openai.com/blog/adversarial-example-research.
              </li>

              <li>
                  Guoliang.Kang. "Random Erasing Data Augmentation." GroundAI, GroundAI, 16 Aug. 2017, www.groundai.com/
                  project/random-erasing-data-augmentation/2.
              </li>

              <li>
                  Hendrycks, Dan. "Natural Adversarial Examples." Arxiv, 2020.
              </li>

              <li>
                  Yuan, Xiaoyong, et al. "Adversarial Examples: Attacks and Defenses for Deep Learning." IEEE
                  Transactions on Neural Networks and Learning Systems, vol. 30, no. 9, 2019, pp. 2805-2824., doi:10.1109/
                  tnnls.2018.2886017.
              </li>
          </ol>
      </div>
  </div>



  <!-- Bootstrap JS -->
  <script src="assets/jquery-3.4.1.slim.min.js"></script>
  <script src="assets/popper.min.js"></script>
  <script src="assets/bootstrap.min.js"></script>
  <!-- MathJax -->
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  </body>

</html>